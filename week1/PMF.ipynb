{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Matrix Factorization\n",
    "\n",
    "- paper: papers.nips.cc/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC\n",
    " - monte carlo markov chain\n",
    " - monte carlo: 몬테카를로 방법(Monte Carlo method) (또는 몬테카를로 실험)은 반복된 무작위 추출(repeated random sampling)을 이용하여 함수의 값을 수리적으로 근사하는 알고리즘을 부르는 용어이다. -> 통계적인 특성을 이용해 무수히 뭔가를 많이 시도해본다”\n",
    " - markov chain: Markov Chain은 어떤 상태에서 다른 상태로 넘어갈 때, 바로 전 단계의 상태에만 영향을 받는 확률 과정을 의미한다.\n",
    " -MCMC: MCMC를 수행한다는 것은 첫 샘플을 랜덤하게 선정한 뒤, 첫 샘플에 의해 그 다음번 샘플이 추천되는 방식의 시도를 무수하게 해본다는 의미를 갖고 있다.\n",
    "\n",
    "### Rejection sampling\n",
    "- sampling: 확률분포의 확률밀도에 맞게 sample을 생성하는 것 \n",
    "- 수학적으로는 CDF의 역함수 연산을 수행하는 것과 같다\n",
    "- 제안 분포: rejection sampling의 핵심아이디어\n",
    "- 제안 분포에서 샘플 추출, 추출된 샘플이 타겟 분포에서 나올 확률 검토 후 기각여부 판단\n",
    "- 기각여부 판단은 f(x)/Mg(x)가 특정 값보다 큰지 작은지 확인, 이때 Mg(x)는 제안분포의 상수배\n",
    "\n",
    "참고:  https://angeloyeo.github.io/2020/09/17/MCMC.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retricted boltzman machine\n",
    "Restricted Boltzmann Machine(이하 RBM)은 Generative Model이라고 하는데, ANN, DNN, CNN, RNN 등과 같은 Deterministic Model들은 \n",
    "타겟과 가설 간의 차이를 줄여서 오차를 줄이는 것이 목표라고 한다면, Generative Model들의 목표는 확률밀도함수를 모델링하는 것이다.\n",
    "대표적으로 GAN이 있을 것이다. 확률밀도함수를 통해 결과물(여기서는 얼굴)을 생성해주는 과정을 샘플링(sampling)이라고 한다.\n",
    "즉, Generative Model의 목적은 확률분포를 정확히 학습해 좋은 sample을 sampling하는 것이라고 정리할 수 있을 것이다.\n",
    "\n",
    "#### Boltzmann machine\n",
    "볼츠만 머신은 확률분포(pmf, pdf)를 학습하기 위해 만들어졌다. Boltzmann Machine이 가정하는 것은 “우리가 보고 있는 것들 외에도 보이지 않는 요소들까지 잘 포함시켜 학습할 수 있다면 확률분포를 좀 더 정확하게 알 수 있지 않을까?”라는 것이다.\n",
    "Boltzmann macine은 hidden unit과 visible unit으로 구성되어있는데 이는 각각이 특성을 의미한다. \n",
    "\n",
    "#### restircted BM\n",
    "RBM은 BM과 다르게 각 unit간 연결이 visible과 hidden 사이에만 존해하는 biparite graph형태이다. \n",
    "각 layer간 내부연결이 없는 것은 사건 간 독립성을 가정, 확률분포의 결합을 더 쉽게 하기 위해서이다. \n",
    "또한 visibile layer와 hidden layer만을 연결함으로서 이들 중 하나의 값이 주어졌을 때 데이터를 계산할 수 있도록 하는 조건부 확율을 계산하게끔 해주기 위해서이다. \n",
    "- 이는 RBM이 Feed forward neural netwark처럼 학습하게 해준다는 독특한 특징을 가지게 한다. \n",
    "- RBM의 작동방식은 FFNN과 유사하게 forward propagation을 통해 hidden unit의 상태를 결정하고, 다시 hidden unit의 상태로부터 back propagation을 함으로써 visible unit의 상태를 재결정하게 된다.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](img/RBM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- visible layer: 입력 데이터가 들어가는 곳. 각 입력 데이터들은 여러가지 상태(state)를 가질 수 있음. 앞으로 표기는 v\n",
    "- hidden layer: 은닉 데이터가 샘플링되어 나오는 곳. 각 은닉 데이터들은 여러가지 상태를 가질 수 있음. 앞으로 표기는 h\n",
    "- weight matrix: visible layer와 hidden layer를 연결해주는 장치. 원래의 Boltzmann Machine에서부터 파생되어 나온 개념. 앞으로 표기는 W\n",
    "- bias for visible layer: 입력 데이터의 내재적 특성(inherent property)을 설정해주는 부분. 후술하겠지만, 어떤 visible unit이 거의 항상 1인 경우라면 해당 unit의 bias는 높을 수록 좋음. 앞으로 표기는 b\n",
    "\n",
    "- bias for hidden layer: 은닉 데이터의 내재적 특성을 설정해주는 부분. 위의 visible layer의 bias와 유사한 역할. 앞으로 표기는 c\n",
    "\n",
    "W는 행렬, 나머지는 벡터형태이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
